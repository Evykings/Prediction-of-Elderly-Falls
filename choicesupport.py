# -*- coding: utf-8 -*-
"""choicesupport.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AZbnO0zykx7_QWwLuD801gE5tdg8sERD
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.metrics import classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score
from time import time as tt

"""Reading the Datasets into python"""

# Reading the dataset into Python
df = pd.read_csv('cStick.csv')


"""Printing the first 5 entering of the dataset"""

# Displaying the first 5 dataset
df.head(5)

# Printing out all the columns
print(df.columns)

# Renaming the Decision column
df = df.rename(columns={'Decision ': 'Decision'})

#CHECKING FOR MISSING DATA
dfm= df.isnull().sum()
print("Missing Data:\n", dfm)

# Viewing the shape of the dataset
df.shape

# Describe function gives us the statistical values of the numerical columns
df.describe()

"""Distribution of Oxygen Saturation"""

#Data distribution
#plot histograms for Saturation of oxygen (SpO2) column
plt.figure(figsize=(12, 6))
sns.histplot(df['SpO2'], bins=50, kde=True, color='blue', label='Sp02')
plt.title('Distribution of Oxygen Saturation (Sp02) Features')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.show()

"""Distribution of Heart Rate Variability"""

#Data distribution
#plot histograms for Distribution of Heart Rate Variability (HRV) column
plt.figure(figsize=(12, 6))
sns.histplot(df['HRV'], bins=50, kde=True, color='yellow', label='HRV')
plt.title('Distribution of Heart Rate Variability (HRV) Features')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.show()

"""Distribution of Sugar level"""

#Data distribution
#plot histograms for the Distribution of Sugar Level
plt.figure(figsize=(12, 6))
sns.histplot(df['Sugar level'], bins=50, kde=True,  label='Sugar level')
plt.title('Distribution of Sugar level Features')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.show()

# let's list all numerical features
numerical_columns= ['Distance', 'Pressure', 'HRV', 'Sugar level', 'SpO2', 'Accelerometer']

# Initializing a new dataset with all numerical columns
numerical_columns_df= df[['Distance', 'Pressure', 'HRV', 'Sugar level', 'SpO2', 'Accelerometer']]

#plt.style.use('seaborn')
plt.scatter(df['SpO2'], df['HRV'], c = df['Sugar level'], vmin=0, vmax=100, cmap='rainbow')
plt.colorbar(label='Sugar_level')
plt.xlabel('Distribution of Oxygen Saturation (Sp02)')
plt.ylabel('Heart Rate Variability (HRV)')
plt.title('HRV vs SpO2')
plt.show()

# Correlation Heatmap to indentify features that correlate
import seaborn as sns
correlation_matrix = numerical_columns_df.corr()
# Set the figure size
plt.figure(figsize=(12, 8))

# Create the heatmap
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")

# Set the title
plt.title('Correlation Heatmap')

# Show the plot
plt.show()

df.head(5)

# Rename the Decision column
df.loc[df['Decision'] == 0,'Decision'] = 'no fall detected'
df.loc[df['Decision'] == 1,'Decision'] = 'prediction of fall'
df.loc[df['Decision'] == 2,'Decision'] = 'definite fall'

# Creating the training dataset for the machine learning implementation
X = df.drop('Decision', axis=1)
print(X)

# Creating the Label dataset for the machine learning implementation
Y = df['Decision']
print(Y)

# Count of the occurrence of each label
print(Y.value_counts())

# Spliting the dataset with 60% for training and 40% testing
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=42)

"""Creating a column Tranformer to Scale all the numerical column"""

from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer

# create column transformer
ct = ColumnTransformer(
    [
        (
            "scaling", # --> name of the transformation
            StandardScaler(), # --> main function to apply
            numerical_columns, #-->columns to apply it to (we can give more than one column at once!)
        ),
    ],
    remainder="passthrough", #--> what to do with the non-transformed columns. passthrough=keep them
    verbose_feature_names_out=False #--> this keeps columns names simple.
)

"""Creating a machine learining pipeline to perform Stochastic Gradient Descent classification"""

# Create the machine learning pipeline for SGDClassifier
clf_ = Pipeline(
        steps =[('ct', ct),
              ('classifier', SGDClassifier(loss='log_loss', random_state=0)),
      ]
    )

# Set up the hyper-parameters to test
parameters = {'classifier__eta0': np.logspace(-5,-1,3), # the initial learning rate value (try printing np.logspace(-6,-1,5) to see what comes up...)
              'classifier__learning_rate': ['constant'],#, different strategies for the learning rate across iterations, check documentations for more details
              'classifier__class_weight': ['balanced', None],
              'classifier__alpha': [0.0001, 0.01] # Constant that multiplies the regularization term. The higher the value, the stronger the regularization.
             }

# create the GridSearch function

clf_search = GridSearchCV(clf_, parameters, scoring= "accuracy", cv= 5)

# Also note the parameter cv: this controls how many folds we want to use.
# cv=5 means we are doing 5-folds cross-validation

# Fit all the possible hyper-parameters combinations using cross-validation:
# original data
# let's also time the process
t0 = tt()
_ =clf_search.fit(X_train, y_train)
print(f'Time taken to train gridsearch: {tt()-t0:.2f} seconds.')

# Get the best estimator for further analysis of the results using the test set

best_clf = clf_search.best_estimator_

"""In relationship to the decision of falls (0- no fall detected, 1- person slipped/tripped/prediction of fall and 2- definite fall)"""

#Compute predictions and evaluation metrics using the best estimator
# original data
y_pred = best_clf.predict(X_test)
print(classification_report(y_test, y_pred))


cm= ConfusionMatrixDisplay.from_estimator(best_clf, X_test, y_test)